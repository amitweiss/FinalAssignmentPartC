
# Final Assignment - part 3

## Part 1 - Folder FInalA
## Part 2 - Folder FinalB
## Part 3 - Folder FinclC
## Before running this part, take a look on part a (FinalA) and part b (FinalB)

## First stage - load the data 


```python
import pandas as pd
import pickle
import unicodedata
df = pd.read_csv('data_final.csv',header=0, 
                    delimiter=",")
#df = df.ix[1:]
#df = pd.DataFrame(data=df, columns=['text','type'])




```

## Cleaning the data


```python
vocabulary_size = 300
unknown_token = "UNKNOWNTOKEN"
sentence_start_token = "SENTENCESTART"
sentence_end_token = "SENTENCEEND"
line_break= "NEWLINE"
separator= "SEPARATOR"
```


```python
for d in df["type"]:
    text1 = d.replace('\n',' '+ line_break + ' ')
    text1 = text1.replace('--',' '+ separator + ' ')
    text1 = text1.replace('.',' '+sentence_end_token +' '+ sentence_start_token+' ' )
df["type"][2] 


```




    'afromarengo plana jumping spider species genus afromarengo lives south africa'



# Data Preparing

## Split the data into 3 different structures to build the 3 models


```python
wiki_data   = df[df["text"] == "wiki"]
cnn_data    = df[df["text"] == "cnn"]
recipe_data = df[df["text"] == "recipe"]
```


```python
import numpy as np
import theano
import keras 
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import text_to_word_sequence

wiki_string = ""
for wk in wiki_data["type"]:
    wiki_string+= " " + wk
wiki_words = text_to_word_sequence(wiki_string, lower=False, split=" ")

cnn_string = ""
for cn in cnn_data["type"]:
    cnn_string+= " " + cn
cnn_words = text_to_word_sequence(cnn_string, lower=False, split=" ")


recipe_string = ""
for rc in recipe_data["type"]:
    recipe_string+= " " + rc
recipe_words = text_to_word_sequence(recipe_string, lower=False, split=" ")

```

    Using Theano backend.


## Using sklearn vectorizer to create productive vocablury for 3 different list of words(cnn,wiki,recipe)
## Make list of lists that for every 5 words knows the 6 one
## Using keras library to create sequence of word from text


```python
from keras.preprocessing.text import text_to_word_sequence
from sklearn.feature_extraction.text import CountVectorizer
xData = []
yData = []
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features=600
                              ) 
vectorizer.fit_transform(wiki_words)
vocab = vectorizer.get_feature_names()
vocab.append("unknown")
vocab_words = []
for w in wiki_words:
    if w in vocab:
        vocab_words.append(w)
    else:
        vocab_words.append("unknown")
convert_words_to_int = dict()
convert_int_to_words = dict()
for i, c in enumerate(vocab):
    convert_words_to_int[c] = i
for i, c in enumerate(vocab):
    convert_int_to_words[i] = c
num_of_words = len(vocab_words)
num_of_vocav = len(vocab)

for i in range(0, num_of_words - 5, 1):
        list_in = vocab_words[i:i + 5]
        list_out = vocab_words[i + 5]
        xData.append([convert_words_to_int[word] for word in list_in])
        yData.append(convert_words_to_int[list_out])

```


```python
xData2 = []
yData2 = []
vectorizer2 = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features=600
                              ) 
vectorizer2.fit_transform(cnn_words)
vocab2 = vectorizer2.get_feature_names()
vocab2.append("unknown")
vocab_words2 = []
for w in cnn_words:
    if w in vocab2:
        vocab_words2.append(w)
    else:
        vocab_words2.append("unknown")
convert_words_to_int2 = dict()
convert_int_to_words2 = dict()
for i, c in enumerate(vocab2):
    convert_words_to_int2[c] = i
for i, c in enumerate(vocab2):
    convert_int_to_words2[i] = c
num_of_words2 = len(vocab_words2)
num_of_vocav2 = len(vocab2)

for i in range(0, num_of_words2 - 5, 1):
        list_in2 = vocab_words2[i:i + 5]
        list_out2 = vocab_words2[i + 5]
        xData2.append([convert_words_to_int2[word] for word in list_in2])
        yData2.append(convert_words_to_int2[list_out2])
```


```python
xData3 = []
yData3 = []
vectorizer3 = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features=600
                              ) 
vectorizer3.fit_transform(recipe_words)
vocab3 = vectorizer3.get_feature_names()
vocab3.append("unknown")
vocab_words3 = []
for w in recipe_words:
    if w in vocab3:
        vocab_words3.append(w)
    else:
        vocab_words3.append("unknown")
convert_words_to_int3 = dict()
convert_int_to_words3 = dict()
for i, c in enumerate(vocab3):
    convert_words_to_int3[c] = i
for i, c in enumerate(vocab3):
    convert_int_to_words3[i] = c
num_of_words3 = len(vocab_words3)
num_of_vocav3 = len(vocab3)

for i in range(0, num_of_words3 - 5, 1):
        list_in3 = vocab_words3[i:i + 5]
        list_out3 = vocab_words3[i + 5]
        xData3.append([convert_words_to_int3[word] for word in list_in3])
        yData3.append(convert_words_to_int3[list_out3])
```

## Create 3 models for each kind of data
## The model we chose is the simple RNN model
### 5.Create a model with:
### an Embedding input layer with 42 entries
### a SimpleRNN layer with 186 hidde nodes and a relu activation function and identity initialization
### a Dense output layer with a softmax activation function


```python
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten
from keras.layers.wrappers import TimeDistributed
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from keras.layers.recurrent import SimpleRNN
from keras.layers.recurrent import SimpleRNN
import numpy
from keras.utils import np_utils
n_hidden = 186
n_fac = 42
X = numpy.array(xData)  
Y = np_utils.to_categorical(yData) 
model=Sequential([
        Embedding(input_dim=X.shape[0], output_dim=n_fac, input_length=X.shape[1]),
        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),
        Dense(Y.shape[1], activation='softmax')
    ])


X2 = numpy.array(xData2)  
Y2 = np_utils.to_categorical(yData2) 
model2=Sequential([
        Embedding(input_dim=X2.shape[0], output_dim=n_fac, input_length=X2.shape[1]),
        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),
        Dense(Y2.shape[1], activation='softmax')
    ])


X3 = numpy.array(xData3) 
Y3 = np_utils.to_categorical(yData3) 
model3=Sequential([
        Embedding(input_dim=X3.shape[0], output_dim=n_fac, input_length=X3.shape[1]),
        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),
        Dense(Y3.shape[1], activation='softmax')
    ])


```

### Print the summary of the 3 models


```python
model.summary()
model2.summary()
model3.summary()

```

    ____________________________________________________________________________________________________
    Layer (type)                     Output Shape          Param #     Connected to                     
    ====================================================================================================
    embedding_1 (Embedding)          (None, 5, 42)         108570      embedding_input_1[0][0]          
    ____________________________________________________________________________________________________
    simplernn_1 (SimpleRNN)          (None, 186)           42594       embedding_1[0][0]                
    ____________________________________________________________________________________________________
    dense_1 (Dense)                  (None, 601)           112387      simplernn_1[0][0]                
    ====================================================================================================
    Total params: 263,551
    Trainable params: 263,551
    Non-trainable params: 0
    ____________________________________________________________________________________________________
    ____________________________________________________________________________________________________
    Layer (type)                     Output Shape          Param #     Connected to                     
    ====================================================================================================
    embedding_2 (Embedding)          (None, 5, 42)         118566      embedding_input_2[0][0]          
    ____________________________________________________________________________________________________
    simplernn_2 (SimpleRNN)          (None, 186)           42594       embedding_2[0][0]                
    ____________________________________________________________________________________________________
    dense_2 (Dense)                  (None, 601)           112387      simplernn_2[0][0]                
    ====================================================================================================
    Total params: 273,547
    Trainable params: 273,547
    Non-trainable params: 0
    ____________________________________________________________________________________________________
    ____________________________________________________________________________________________________
    Layer (type)                     Output Shape          Param #     Connected to                     
    ====================================================================================================
    embedding_3 (Embedding)          (None, 5, 42)         261828      embedding_input_3[0][0]          
    ____________________________________________________________________________________________________
    simplernn_3 (SimpleRNN)          (None, 186)           42594       embedding_3[0][0]                
    ____________________________________________________________________________________________________
    dense_3 (Dense)                  (None, 601)           112387      simplernn_3[0][0]                
    ====================================================================================================
    Total params: 416,809
    Trainable params: 416,809
    Non-trainable params: 0
    ____________________________________________________________________________________________________


### Compile the models and fit them to the data for 100 epochs.


```python
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
model3.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
```


```python
model.fit(X, y=Y, batch_size=200, nb_epoch=100, verbose=1)

```

    Epoch 1/100
    2585/2585 [==============================] - 0s - loss: 5.4738 - acc: 0.3110     
    Epoch 2/100
    2585/2585 [==============================] - 0s - loss: 4.8058 - acc: 0.3412     
    Epoch 3/100
    2585/2585 [==============================] - 0s - loss: 4.7751 - acc: 0.3412     
    Epoch 4/100
    2585/2585 [==============================] - 0s - loss: 4.7125 - acc: 0.3412     
    Epoch 5/100
    2585/2585 [==============================] - 0s - loss: 4.6778 - acc: 0.3412     
    Epoch 6/100
    2585/2585 [==============================] - 0s - loss: 4.7016 - acc: 0.3412     
    Epoch 7/100
    2585/2585 [==============================] - 0s - loss: 4.6526 - acc: 0.3412     
    Epoch 8/100
    2585/2585 [==============================] - 0s - loss: 4.6204 - acc: 0.3412     
    Epoch 9/100
    2585/2585 [==============================] - 0s - loss: 4.5910 - acc: 0.3412     
    Epoch 10/100
    2585/2585 [==============================] - 0s - loss: 4.5938 - acc: 0.3412     
    Epoch 11/100
    2585/2585 [==============================] - 0s - loss: 4.5273 - acc: 0.3412     
    Epoch 12/100
    2585/2585 [==============================] - 0s - loss: 4.5065 - acc: 0.3412     
    Epoch 13/100
    2585/2585 [==============================] - 0s - loss: 4.4772 - acc: 0.3416     
    Epoch 14/100
    2585/2585 [==============================] - 0s - loss: 4.4410 - acc: 0.3427     
    Epoch 15/100
    2585/2585 [==============================] - 0s - loss: 4.3773 - acc: 0.3427     
    Epoch 16/100
    2585/2585 [==============================] - 0s - loss: 4.3395 - acc: 0.3431     
    Epoch 17/100
    2585/2585 [==============================] - 0s - loss: 4.3520 - acc: 0.3435     
    Epoch 18/100
    2585/2585 [==============================] - 0s - loss: 4.2881 - acc: 0.3435     
    Epoch 19/100
    2585/2585 [==============================] - 0s - loss: 4.2626 - acc: 0.3431     
    Epoch 20/100
    2585/2585 [==============================] - 0s - loss: 4.2206 - acc: 0.3439     
    Epoch 21/100
    2585/2585 [==============================] - 0s - loss: 4.1945 - acc: 0.3443     
    Epoch 22/100
    2585/2585 [==============================] - 0s - loss: 4.1827 - acc: 0.3455     
    Epoch 23/100
    2585/2585 [==============================] - 0s - loss: 4.1325 - acc: 0.3462     
    Epoch 24/100
    2585/2585 [==============================] - 0s - loss: 4.1101 - acc: 0.3458     
    Epoch 25/100
    2585/2585 [==============================] - 0s - loss: 4.0776 - acc: 0.3470     
    Epoch 26/100
    2585/2585 [==============================] - 0s - loss: 4.0031 - acc: 0.3474     
    Epoch 27/100
    2585/2585 [==============================] - 0s - loss: 3.9466 - acc: 0.3497     
    Epoch 28/100
    2585/2585 [==============================] - 0s - loss: 3.9415 - acc: 0.3493     
    Epoch 29/100
    2585/2585 [==============================] - 0s - loss: 3.8701 - acc: 0.3497     
    Epoch 30/100
    2585/2585 [==============================] - 0s - loss: 3.7999 - acc: 0.3540     
    Epoch 31/100
    2585/2585 [==============================] - 0s - loss: 3.7780 - acc: 0.3509     
    Epoch 32/100
    2585/2585 [==============================] - 0s - loss: 3.6536 - acc: 0.3574     
    Epoch 33/100
    2585/2585 [==============================] - 0s - loss: 3.6359 - acc: 0.3563     
    Epoch 34/100
    2585/2585 [==============================] - 0s - loss: 3.5476 - acc: 0.3594     
    Epoch 35/100
    2585/2585 [==============================] - 0s - loss: 3.4753 - acc: 0.3660     
    Epoch 36/100
    2585/2585 [==============================] - 0s - loss: 3.3713 - acc: 0.3714     
    Epoch 37/100
    2585/2585 [==============================] - 0s - loss: 3.2867 - acc: 0.3725     
    Epoch 38/100
    2585/2585 [==============================] - 0s - loss: 3.2453 - acc: 0.3718     
    Epoch 39/100
    2585/2585 [==============================] - 0s - loss: 3.1313 - acc: 0.3783     
    Epoch 40/100
    2585/2585 [==============================] - 0s - loss: 3.0241 - acc: 0.3876     
    Epoch 41/100
    2585/2585 [==============================] - 0s - loss: 2.9494 - acc: 0.3985     
    Epoch 42/100
    2585/2585 [==============================] - 0s - loss: 2.8652 - acc: 0.4043     
    Epoch 43/100
    2585/2585 [==============================] - 0s - loss: 2.7754 - acc: 0.4062     
    Epoch 44/100
    2585/2585 [==============================] - 0s - loss: 2.6576 - acc: 0.4294     
    Epoch 45/100
    2585/2585 [==============================] - 0s - loss: 2.6023 - acc: 0.4368     
    Epoch 46/100
    2585/2585 [==============================] - 0s - loss: 2.5275 - acc: 0.4429     
    Epoch 47/100
    2585/2585 [==============================] - 0s - loss: 2.4484 - acc: 0.4573     
    Epoch 48/100
    2585/2585 [==============================] - 0s - loss: 2.3383 - acc: 0.4770     
    Epoch 49/100
    2585/2585 [==============================] - 0s - loss: 2.2909 - acc: 0.4785     
    Epoch 50/100
    2585/2585 [==============================] - 0s - loss: 2.1893 - acc: 0.4940     
    Epoch 51/100
    2585/2585 [==============================] - 0s - loss: 2.1246 - acc: 0.5072     
    Epoch 52/100
    2585/2585 [==============================] - 0s - loss: 2.0663 - acc: 0.5215     
    Epoch 53/100
    2585/2585 [==============================] - 0s - loss: 1.9764 - acc: 0.5323     
    Epoch 54/100
    2585/2585 [==============================] - 0s - loss: 1.9023 - acc: 0.5462     
    Epoch 55/100
    2585/2585 [==============================] - 0s - loss: 1.8197 - acc: 0.5567     
    Epoch 56/100
    2585/2585 [==============================] - 0s - loss: 1.7847 - acc: 0.5737     
    Epoch 57/100
    2585/2585 [==============================] - 0s - loss: 1.6916 - acc: 0.5830     
    Epoch 58/100
    2585/2585 [==============================] - 0s - loss: 1.5985 - acc: 0.6124     
    Epoch 59/100
    2585/2585 [==============================] - 0s - loss: 1.6273 - acc: 0.6043     
    Epoch 60/100
    2585/2585 [==============================] - 0s - loss: 1.5569 - acc: 0.6143     
    Epoch 61/100
    2585/2585 [==============================] - 0s - loss: 1.4064 - acc: 0.6538     
    Epoch 62/100
    2585/2585 [==============================] - 0s - loss: 1.4708 - acc: 0.6344     
    Epoch 63/100
    2585/2585 [==============================] - 0s - loss: 1.3722 - acc: 0.6530     
    Epoch 64/100
    2585/2585 [==============================] - 0s - loss: 1.3404 - acc: 0.6685     
    Epoch 65/100
    2585/2585 [==============================] - 0s - loss: 1.2567 - acc: 0.6909     
    Epoch 66/100
    2585/2585 [==============================] - 0s - loss: 1.2432 - acc: 0.6886     
    Epoch 67/100
    2585/2585 [==============================] - 0s - loss: 1.2063 - acc: 0.7037     
    Epoch 68/100
    2585/2585 [==============================] - 0s - loss: 1.1849 - acc: 0.6979     
    Epoch 69/100
    2585/2585 [==============================] - 0s - loss: 1.1150 - acc: 0.7296     
    Epoch 70/100
    2585/2585 [==============================] - 0s - loss: 1.0558 - acc: 0.7373     
    Epoch 71/100
    2585/2585 [==============================] - 0s - loss: 1.0864 - acc: 0.7188     
    Epoch 72/100
    2585/2585 [==============================] - 0s - loss: 1.0649 - acc: 0.7234     
    Epoch 73/100
    2585/2585 [==============================] - 0s - loss: 0.8922 - acc: 0.7783     
    Epoch 74/100
    2585/2585 [==============================] - 0s - loss: 0.9768 - acc: 0.7513     
    Epoch 75/100
    2585/2585 [==============================] - 0s - loss: 0.9361 - acc: 0.7574     
    Epoch 76/100
    2585/2585 [==============================] - 0s - loss: 0.8548 - acc: 0.7818     
    Epoch 77/100
    2585/2585 [==============================] - 0s - loss: 0.9397 - acc: 0.7675     
    Epoch 78/100
    2585/2585 [==============================] - 0s - loss: 0.7952 - acc: 0.8066     
    Epoch 79/100
    2585/2585 [==============================] - 0s - loss: 0.7907 - acc: 0.8066     
    Epoch 80/100
    2585/2585 [==============================] - 0s - loss: 0.8688 - acc: 0.7787     
    Epoch 81/100
    2585/2585 [==============================] - 0s - loss: 0.7066 - acc: 0.8244     
    Epoch 82/100
    2585/2585 [==============================] - 0s - loss: 0.8011 - acc: 0.7919     
    Epoch 83/100
    2585/2585 [==============================] - 0s - loss: 0.7140 - acc: 0.8209     
    Epoch 84/100
    2585/2585 [==============================] - 0s - loss: 0.6928 - acc: 0.8302     
    Epoch 85/100
    2585/2585 [==============================] - 0s - loss: 0.7191 - acc: 0.8159     
    Epoch 86/100
    2585/2585 [==============================] - 0s - loss: 0.6541 - acc: 0.8464     
    Epoch 87/100
    2585/2585 [==============================] - 0s - loss: 0.6165 - acc: 0.8545     
    Epoch 88/100
    2585/2585 [==============================] - 0s - loss: 0.7395 - acc: 0.8031     
    Epoch 89/100
    2585/2585 [==============================] - 0s - loss: 0.5745 - acc: 0.8611     
    Epoch 90/100
    2585/2585 [==============================] - 0s - loss: 0.6353 - acc: 0.8391     
    Epoch 91/100
    2585/2585 [==============================] - 0s - loss: 0.5666 - acc: 0.8596     
    Epoch 92/100
    2585/2585 [==============================] - 0s - loss: 0.5630 - acc: 0.8573     
    Epoch 93/100
    2585/2585 [==============================] - 0s - loss: 0.6616 - acc: 0.8275     
    Epoch 94/100
    2585/2585 [==============================] - 0s - loss: 0.4921 - acc: 0.8824     
    Epoch 95/100
    2585/2585 [==============================] - 0s - loss: 0.5188 - acc: 0.8700     
    Epoch 96/100
    2585/2585 [==============================] - 0s - loss: 0.6283 - acc: 0.8251     
    Epoch 97/100
    2585/2585 [==============================] - 0s - loss: 0.4497 - acc: 0.8928     
    Epoch 98/100
    2585/2585 [==============================] - 0s - loss: 0.6002 - acc: 0.8460     
    Epoch 99/100
    2585/2585 [==============================] - 0s - loss: 0.4605 - acc: 0.8836     
    Epoch 100/100
    2585/2585 [==============================] - 0s - loss: 0.4176 - acc: 0.9002     





    <keras.callbacks.History at 0x11740a990>




```python
model2.fit(X2, y=Y2, batch_size=200, nb_epoch=100, verbose=1)

```

    Epoch 1/100
    2823/2823 [==============================] - 0s - loss: 5.8394 - acc: 0.1923     
    Epoch 2/100
    2823/2823 [==============================] - 0s - loss: 5.3878 - acc: 0.2111     
    Epoch 3/100
    2823/2823 [==============================] - 0s - loss: 5.2779 - acc: 0.2111     
    Epoch 4/100
    2823/2823 [==============================] - 0s - loss: 5.2020 - acc: 0.2111     
    Epoch 5/100
    2823/2823 [==============================] - 0s - loss: 5.0985 - acc: 0.2129     
    Epoch 6/100
    2823/2823 [==============================] - 0s - loss: 4.9147 - acc: 0.2193     
    Epoch 7/100
    2823/2823 [==============================] - 0s - loss: 4.7981 - acc: 0.2196     
    Epoch 8/100
    2823/2823 [==============================] - 0s - loss: 4.6732 - acc: 0.2256     
    Epoch 9/100
    2823/2823 [==============================] - 0s - loss: 4.5498 - acc: 0.2299     
    Epoch 10/100
    2823/2823 [==============================] - 0s - loss: 4.4174 - acc: 0.2370     
    Epoch 11/100
    2823/2823 [==============================] - 0s - loss: 4.2448 - acc: 0.2490     
    Epoch 12/100
    2823/2823 [==============================] - 0s - loss: 4.0266 - acc: 0.2678     
    Epoch 13/100
    2823/2823 [==============================] - 0s - loss: 3.8746 - acc: 0.2859     
    Epoch 14/100
    2823/2823 [==============================] - 0s - loss: 3.6011 - acc: 0.3181     
    Epoch 15/100
    2823/2823 [==============================] - 0s - loss: 3.4047 - acc: 0.3482     
    Epoch 16/100
    2823/2823 [==============================] - 0s - loss: 3.1750 - acc: 0.3826     
    Epoch 17/100
    2823/2823 [==============================] - 0s - loss: 2.9483 - acc: 0.4166     
    Epoch 18/100
    2823/2823 [==============================] - 0s - loss: 2.7642 - acc: 0.4573     
    Epoch 19/100
    2823/2823 [==============================] - 0s - loss: 2.6665 - acc: 0.4782     
    Epoch 20/100
    2823/2823 [==============================] - 0s - loss: 2.4757 - acc: 0.5097     
    Epoch 21/100
    2823/2823 [==============================] - 0s - loss: 2.3343 - acc: 0.5345     
    Epoch 22/100
    2823/2823 [==============================] - 0s - loss: 2.2532 - acc: 0.5515     
    Epoch 23/100
    2823/2823 [==============================] - 0s - loss: 2.1087 - acc: 0.5859     
    Epoch 24/100
    2823/2823 [==============================] - 0s - loss: 2.0438 - acc: 0.5831     
    Epoch 25/100
    2823/2823 [==============================] - 0s - loss: 1.9456 - acc: 0.6114     
    Epoch 26/100
    2823/2823 [==============================] - 0s - loss: 1.8479 - acc: 0.6259     
    Epoch 27/100
    2823/2823 [==============================] - 0s - loss: 1.8202 - acc: 0.6242     
    Epoch 28/100
    2823/2823 [==============================] - 0s - loss: 1.7438 - acc: 0.6440     
    Epoch 29/100
    2823/2823 [==============================] - 0s - loss: 1.6638 - acc: 0.6635     
    Epoch 30/100
    2823/2823 [==============================] - 0s - loss: 1.5915 - acc: 0.6674     
    Epoch 31/100
    2823/2823 [==============================] - 0s - loss: 1.5915 - acc: 0.6585     
    Epoch 32/100
    2823/2823 [==============================] - 0s - loss: 1.5046 - acc: 0.6805     
    Epoch 33/100
    2823/2823 [==============================] - 0s - loss: 1.4403 - acc: 0.6872     
    Epoch 34/100
    2823/2823 [==============================] - 0s - loss: 1.4134 - acc: 0.6915     
    Epoch 35/100
    2823/2823 [==============================] - 0s - loss: 1.3641 - acc: 0.7028     
    Epoch 36/100
    2823/2823 [==============================] - 0s - loss: 1.3557 - acc: 0.6900     
    Epoch 37/100
    2823/2823 [==============================] - 0s - loss: 1.2714 - acc: 0.7145     
    Epoch 38/100
    2823/2823 [==============================] - 0s - loss: 1.2845 - acc: 0.7102     
    Epoch 39/100
    2823/2823 [==============================] - 0s - loss: 1.2058 - acc: 0.7226     
    Epoch 40/100
    2823/2823 [==============================] - 0s - loss: 1.2664 - acc: 0.6996     
    Epoch 41/100
    2823/2823 [==============================] - 0s - loss: 1.1232 - acc: 0.7407     
    Epoch 42/100
    2823/2823 [==============================] - 0s - loss: 1.1338 - acc: 0.7318     
    Epoch 43/100
    2823/2823 [==============================] - 0s - loss: 1.0752 - acc: 0.7403     
    Epoch 44/100
    2823/2823 [==============================] - 0s - loss: 1.1422 - acc: 0.7180     
    Epoch 45/100
    2823/2823 [==============================] - 0s - loss: 1.0238 - acc: 0.7531     
    Epoch 46/100
    2823/2823 [==============================] - 0s - loss: 1.0463 - acc: 0.7411     
    Epoch 47/100
    2823/2823 [==============================] - 0s - loss: 0.9879 - acc: 0.7549     
    Epoch 48/100
    2823/2823 [==============================] - 0s - loss: 1.0542 - acc: 0.7403     
    Epoch 49/100
    2823/2823 [==============================] - 0s - loss: 0.9422 - acc: 0.7627     
    Epoch 50/100
    2823/2823 [==============================] - 0s - loss: 0.9076 - acc: 0.7736     
    Epoch 51/100
    2823/2823 [==============================] - 0s - loss: 0.9951 - acc: 0.7474     
    Epoch 52/100
    2823/2823 [==============================] - 0s - loss: 0.8655 - acc: 0.7768     
    Epoch 53/100
    2823/2823 [==============================] - 0s - loss: 0.8801 - acc: 0.7800     
    Epoch 54/100
    2823/2823 [==============================] - 0s - loss: 0.8339 - acc: 0.7853     
    Epoch 55/100
    2823/2823 [==============================] - 0s - loss: 0.8363 - acc: 0.7843     
    Epoch 56/100
    2823/2823 [==============================] - 0s - loss: 0.8591 - acc: 0.7804     
    Epoch 57/100
    2823/2823 [==============================] - 0s - loss: 0.8028 - acc: 0.7889     
    Epoch 58/100
    2823/2823 [==============================] - 0s - loss: 0.8246 - acc: 0.7790     
    Epoch 59/100
    2823/2823 [==============================] - 0s - loss: 0.7672 - acc: 0.8006     
    Epoch 60/100
    2823/2823 [==============================] - 0s - loss: 0.7094 - acc: 0.8091     
    Epoch 61/100
    2823/2823 [==============================] - 0s - loss: 0.7936 - acc: 0.7871     
    Epoch 62/100
    2823/2823 [==============================] - 0s - loss: 0.6969 - acc: 0.8130     
    Epoch 63/100
    2823/2823 [==============================] - 0s - loss: 0.7482 - acc: 0.8002     
    Epoch 64/100
    2823/2823 [==============================] - 0s - loss: 0.6854 - acc: 0.8197     
    Epoch 65/100
    2823/2823 [==============================] - 0s - loss: 0.6782 - acc: 0.8144     
    Epoch 66/100
    2823/2823 [==============================] - 0s - loss: 0.6633 - acc: 0.8211     
    Epoch 67/100
    2823/2823 [==============================] - 0s - loss: 0.6597 - acc: 0.8264     
    Epoch 68/100
    2823/2823 [==============================] - 0s - loss: 0.6456 - acc: 0.8278     
    Epoch 69/100
    2823/2823 [==============================] - 0s - loss: 0.6077 - acc: 0.8328     
    Epoch 70/100
    2823/2823 [==============================] - 0s - loss: 0.6000 - acc: 0.8371     
    Epoch 71/100
    2823/2823 [==============================] - 0s - loss: 0.5901 - acc: 0.8409     
    Epoch 72/100
    2823/2823 [==============================] - 0s - loss: 0.6222 - acc: 0.8296     
    Epoch 73/100
    2823/2823 [==============================] - 0s - loss: 0.5574 - acc: 0.8480     
    Epoch 74/100
    2823/2823 [==============================] - 0s - loss: 0.6032 - acc: 0.8339     
    Epoch 75/100
    2823/2823 [==============================] - 0s - loss: 0.5495 - acc: 0.8498     
    Epoch 76/100
    2823/2823 [==============================] - 0s - loss: 0.6072 - acc: 0.8378     
    Epoch 77/100
    2823/2823 [==============================] - 0s - loss: 0.5099 - acc: 0.8622     
    Epoch 78/100
    2823/2823 [==============================] - 0s - loss: 0.5577 - acc: 0.8445     
    Epoch 79/100
    2823/2823 [==============================] - 0s - loss: 0.5572 - acc: 0.8498     
    Epoch 80/100
    2823/2823 [==============================] - 0s - loss: 0.4745 - acc: 0.8686     
    Epoch 81/100
    2823/2823 [==============================] - 0s - loss: 0.4999 - acc: 0.8590     
    Epoch 82/100
    2823/2823 [==============================] - 0s - loss: 0.4837 - acc: 0.8618     
    Epoch 83/100
    2823/2823 [==============================] - 0s - loss: 0.4826 - acc: 0.8654     
    Epoch 84/100
    2823/2823 [==============================] - 0s - loss: 0.4906 - acc: 0.8622     
    Epoch 85/100
    2823/2823 [==============================] - 0s - loss: 0.4504 - acc: 0.8746     
    Epoch 86/100
    2823/2823 [==============================] - 0s - loss: 0.4688 - acc: 0.8672     
    Epoch 87/100
    2823/2823 [==============================] - 0s - loss: 0.4306 - acc: 0.8813     
    Epoch 88/100
    2823/2823 [==============================] - 0s - loss: 0.4973 - acc: 0.8523     
    Epoch 89/100
    2823/2823 [==============================] - 0s - loss: 0.4109 - acc: 0.8845     
    Epoch 90/100
    2823/2823 [==============================] - 0s - loss: 0.4262 - acc: 0.8771     
    Epoch 91/100
    2823/2823 [==============================] - 0s - loss: 0.4074 - acc: 0.8785     
    Epoch 92/100
    2823/2823 [==============================] - 0s - loss: 0.4278 - acc: 0.8704     
    Epoch 93/100
    2823/2823 [==============================] - 0s - loss: 0.3914 - acc: 0.8895     
    Epoch 94/100
    2823/2823 [==============================] - 0s - loss: 0.3893 - acc: 0.8852     
    Epoch 95/100
    2823/2823 [==============================] - 0s - loss: 0.3853 - acc: 0.8913     
    Epoch 96/100
    2823/2823 [==============================] - 0s - loss: 0.3806 - acc: 0.8927     
    Epoch 97/100
    2823/2823 [==============================] - 0s - loss: 0.3795 - acc: 0.8930     
    Epoch 98/100
    2823/2823 [==============================] - 0s - loss: 0.3744 - acc: 0.8920     
    Epoch 99/100
    2823/2823 [==============================] - 0s - loss: 0.3717 - acc: 0.8891     
    Epoch 100/100
    2823/2823 [==============================] - 0s - loss: 0.3443 - acc: 0.9012     





    <keras.callbacks.History at 0x12140cb10>




```python
model3.fit(X3, y=Y3, batch_size=200, nb_epoch=100, verbose=1)
```

    Epoch 1/100
    6234/6234 [==============================] - 0s - loss: 5.0379 - acc: 0.1833     
    Epoch 2/100
    6234/6234 [==============================] - 0s - loss: 4.7024 - acc: 0.1888     
    Epoch 3/100
    6234/6234 [==============================] - 0s - loss: 4.6433 - acc: 0.1888     
    Epoch 4/100
    6234/6234 [==============================] - 0s - loss: 4.5834 - acc: 0.1878     
    Epoch 5/100
    6234/6234 [==============================] - 0s - loss: 4.4776 - acc: 0.1893     
    Epoch 6/100
    6234/6234 [==============================] - 0s - loss: 4.3784 - acc: 0.1960     
    Epoch 7/100
    6234/6234 [==============================] - 0s - loss: 4.2683 - acc: 0.1970     
    Epoch 8/100
    6234/6234 [==============================] - 0s - loss: 4.1895 - acc: 0.1996     
    Epoch 9/100
    6234/6234 [==============================] - 0s - loss: 4.0878 - acc: 0.2056     
    Epoch 10/100
    6234/6234 [==============================] - 0s - loss: 3.9989 - acc: 0.2114     
    Epoch 11/100
    6234/6234 [==============================] - 0s - loss: 3.9002 - acc: 0.2239     
    Epoch 12/100
    6234/6234 [==============================] - 0s - loss: 3.8163 - acc: 0.2324     
    Epoch 13/100
    6234/6234 [==============================] - 0s - loss: 3.7257 - acc: 0.2488     
    Epoch 14/100
    6234/6234 [==============================] - 0s - loss: 3.6217 - acc: 0.2546     
    Epoch 15/100
    6234/6234 [==============================] - 0s - loss: 3.5311 - acc: 0.2640     
    Epoch 16/100
    6234/6234 [==============================] - 0s - loss: 3.4488 - acc: 0.2719     
    Epoch 17/100
    6234/6234 [==============================] - 0s - loss: 3.3566 - acc: 0.2807     
    Epoch 18/100
    6234/6234 [==============================] - 0s - loss: 3.2678 - acc: 0.2915     
    Epoch 19/100
    6234/6234 [==============================] - 0s - loss: 3.1787 - acc: 0.3009     
    Epoch 20/100
    6234/6234 [==============================] - 0s - loss: 3.0919 - acc: 0.3131     
    Epoch 21/100
    6234/6234 [==============================] - 0s - loss: 3.0034 - acc: 0.3194     
    Epoch 22/100
    6234/6234 [==============================] - 0s - loss: 2.9069 - acc: 0.3306     
    Epoch 23/100
    6234/6234 [==============================] - 0s - loss: 2.8210 - acc: 0.3431     
    Epoch 24/100
    6234/6234 [==============================] - 0s - loss: 2.7340 - acc: 0.3503     
    Epoch 25/100
    6234/6234 [==============================] - 0s - loss: 2.6417 - acc: 0.3657     
    Epoch 26/100
    6234/6234 [==============================] - 0s - loss: 2.5541 - acc: 0.3712     
    Epoch 27/100
    6234/6234 [==============================] - 0s - loss: 2.4645 - acc: 0.3879     
    Epoch 28/100
    6234/6234 [==============================] - 0s - loss: 2.3894 - acc: 0.3999     
    Epoch 29/100
    6234/6234 [==============================] - 0s - loss: 2.3009 - acc: 0.4113     
    Epoch 30/100
    6234/6234 [==============================] - 0s - loss: 2.2246 - acc: 0.4256     
    Epoch 31/100
    6234/6234 [==============================] - 0s - loss: 2.1555 - acc: 0.4406     
    Epoch 32/100
    6234/6234 [==============================] - 0s - loss: 2.0656 - acc: 0.4613     
    Epoch 33/100
    6234/6234 [==============================] - 0s - loss: 1.9995 - acc: 0.4764     
    Epoch 34/100
    6234/6234 [==============================] - 0s - loss: 1.9267 - acc: 0.4859     
    Epoch 35/100
    6234/6234 [==============================] - 0s - loss: 1.8573 - acc: 0.5077     
    Epoch 36/100
    6234/6234 [==============================] - 0s - loss: 1.7975 - acc: 0.5172     
    Epoch 37/100
    6234/6234 [==============================] - 0s - loss: 1.7244 - acc: 0.5417     
    Epoch 38/100
    6234/6234 [==============================] - 0s - loss: 1.6843 - acc: 0.5465     
    Epoch 39/100
    6234/6234 [==============================] - 0s - loss: 1.6287 - acc: 0.5566     
    Epoch 40/100
    6234/6234 [==============================] - 0s - loss: 1.5740 - acc: 0.5687     
    Epoch 41/100
    6234/6234 [==============================] - 0s - loss: 1.5168 - acc: 0.5831     
    Epoch 42/100
    6234/6234 [==============================] - 0s - loss: 1.4706 - acc: 0.5946     
    Epoch 43/100
    6234/6234 [==============================] - 0s - loss: 1.4233 - acc: 0.6036     
    Epoch 44/100
    6234/6234 [==============================] - 0s - loss: 1.3807 - acc: 0.6182     
    Epoch 45/100
    6234/6234 [==============================] - 0s - loss: 1.3400 - acc: 0.6291     
    Epoch 46/100
    6234/6234 [==============================] - 0s - loss: 1.2793 - acc: 0.6444     
    Epoch 47/100
    6234/6234 [==============================] - 0s - loss: 1.2654 - acc: 0.6432     
    Epoch 48/100
    6234/6234 [==============================] - 0s - loss: 1.2046 - acc: 0.6517     
    Epoch 49/100
    6234/6234 [==============================] - 1s - loss: 1.1646 - acc: 0.6707     
    Epoch 50/100
    6234/6234 [==============================] - 1s - loss: 1.1299 - acc: 0.6739     
    Epoch 51/100
    6234/6234 [==============================] - 1s - loss: 1.0900 - acc: 0.6824     
    Epoch 52/100
    6234/6234 [==============================] - 0s - loss: 1.0716 - acc: 0.6869     
    Epoch 53/100
    6234/6234 [==============================] - 0s - loss: 1.0279 - acc: 0.7098     
    Epoch 54/100
    6234/6234 [==============================] - 1s - loss: 0.9860 - acc: 0.7185     
    Epoch 55/100
    6234/6234 [==============================] - 1s - loss: 0.9659 - acc: 0.7190     
    Epoch 56/100
    6234/6234 [==============================] - 1s - loss: 0.9348 - acc: 0.7350     
    Epoch 57/100
    6234/6234 [==============================] - 1s - loss: 0.8851 - acc: 0.7406     
    Epoch 58/100
    6234/6234 [==============================] - 1s - loss: 0.8700 - acc: 0.7421     
    Epoch 59/100
    6234/6234 [==============================] - 1s - loss: 0.8492 - acc: 0.7483     
    Epoch 60/100
    6234/6234 [==============================] - 1s - loss: 0.8119 - acc: 0.7640     
    Epoch 61/100
    6234/6234 [==============================] - 1s - loss: 0.7920 - acc: 0.7648     
    Epoch 62/100
    6234/6234 [==============================] - 1s - loss: 0.7689 - acc: 0.7708     
    Epoch 63/100
    6234/6234 [==============================] - 1s - loss: 0.7357 - acc: 0.7839     
    Epoch 64/100
    6234/6234 [==============================] - 1s - loss: 0.7347 - acc: 0.7807     
    Epoch 65/100
    6234/6234 [==============================] - 1s - loss: 0.6939 - acc: 0.7913     
    Epoch 66/100
    6234/6234 [==============================] - 1s - loss: 0.6758 - acc: 0.7980     
    Epoch 67/100
    6234/6234 [==============================] - 1s - loss: 0.6681 - acc: 0.7977     
    Epoch 68/100
    6234/6234 [==============================] - 1s - loss: 0.6446 - acc: 0.8033     
    Epoch 69/100
    6234/6234 [==============================] - 1s - loss: 0.6195 - acc: 0.8189     
    Epoch 70/100
    6234/6234 [==============================] - 1s - loss: 0.6067 - acc: 0.8186     
    Epoch 71/100
    6234/6234 [==============================] - 1s - loss: 0.6002 - acc: 0.8149     
    Epoch 72/100
    6234/6234 [==============================] - 1s - loss: 0.5616 - acc: 0.8308     
    Epoch 73/100
    6234/6234 [==============================] - 1s - loss: 0.5678 - acc: 0.8304     
    Epoch 74/100
    6234/6234 [==============================] - 1s - loss: 0.5476 - acc: 0.8325     
    Epoch 75/100
    6234/6234 [==============================] - 1s - loss: 0.5260 - acc: 0.8410     
    Epoch 76/100
    6234/6234 [==============================] - 1s - loss: 0.5132 - acc: 0.8489     
    Epoch 77/100
    6234/6234 [==============================] - 1s - loss: 0.5110 - acc: 0.8447     
    Epoch 78/100
    6234/6234 [==============================] - 1s - loss: 0.4989 - acc: 0.8484     
    Epoch 79/100
    6234/6234 [==============================] - 1s - loss: 0.4798 - acc: 0.8521     
    Epoch 80/100
    6234/6234 [==============================] - 1s - loss: 0.4744 - acc: 0.8551     
    Epoch 81/100
    6234/6234 [==============================] - 1s - loss: 0.4591 - acc: 0.8564     
    Epoch 82/100
    6234/6234 [==============================] - 1s - loss: 0.4492 - acc: 0.8630     
    Epoch 83/100
    6234/6234 [==============================] - 1s - loss: 0.4369 - acc: 0.8649     
    Epoch 84/100
    6234/6234 [==============================] - 1s - loss: 0.4311 - acc: 0.8648     
    Epoch 85/100
    6234/6234 [==============================] - 1s - loss: 0.4157 - acc: 0.8720     
    Epoch 86/100
    6234/6234 [==============================] - 1s - loss: 0.4249 - acc: 0.8672     
    Epoch 87/100
    6234/6234 [==============================] - 1s - loss: 0.3926 - acc: 0.8799     
    Epoch 88/100
    6234/6234 [==============================] - 1s - loss: 0.3964 - acc: 0.8744     
    Epoch 89/100
    6234/6234 [==============================] - 1s - loss: 0.3871 - acc: 0.8805     
    Epoch 90/100
    6234/6234 [==============================] - 1s - loss: 0.3830 - acc: 0.8762     
    Epoch 91/100
    6234/6234 [==============================] - 1s - loss: 0.3740 - acc: 0.8811     
    Epoch 92/100
    6234/6234 [==============================] - 1s - loss: 0.3708 - acc: 0.8802     
    Epoch 93/100
    6234/6234 [==============================] - 1s - loss: 0.3585 - acc: 0.8848     
    Epoch 94/100
    6234/6234 [==============================] - 1s - loss: 0.3558 - acc: 0.8906     
    Epoch 95/100
    6234/6234 [==============================] - 1s - loss: 0.3433 - acc: 0.8898     
    Epoch 96/100
    6234/6234 [==============================] - 1s - loss: 0.3406 - acc: 0.8914     
    Epoch 97/100
    6234/6234 [==============================] - 1s - loss: 0.3365 - acc: 0.8924     
    Epoch 98/100
    6234/6234 [==============================] - 1s - loss: 0.3314 - acc: 0.8927     
    Epoch 99/100
    6234/6234 [==============================] - 1s - loss: 0.3258 - acc: 0.8909     
    Epoch 100/100
    6234/6234 [==============================] - 1s - loss: 0.3216 - acc: 0.8948     





    <keras.callbacks.History at 0x121e62310>



## Genarate texts from each data using the models we created


```python
answers = []
unk = convert_words_to_int["unknown"]
wikis = []
count_wiki = len(df[df.text == 'wiki'])
for j in range(0, int(0.3*count_wiki)):
    prev5 = np.array([convert_words_to_int[word] for word in vocab_words[j*10:j*10+5]])
    ans = ""
    for i in range(0, 100):
        bla = numpy.reshape(prev5, (1, len(prev5)))
        pred = model.predict(bla, verbose=0)
        word1 = pred.argsort()[0][-2:]
        # print (word1[0])
        if word1[0] == unk:
            word1 = word1[1]
        else:
            word1 = word1[0]
        # print(convert_int_to_words[word1])
        ans += convert_int_to_words[word1] + " "
        prev5 = prev5[1:5]
        prev5 = numpy.append(prev5, word1)
    wikis.append(ans)
    answers.append("wiki")
```


```python
unk2 = convert_words_to_int2["unknown"]
cnns = []
count_cnn = len(df[df.text == 'cnn'])
for j in range(0, int(0.3*count_cnn)):
    prev6 = np.array([convert_words_to_int2[word] for word in vocab_words2[j*10:j*10+5]])
    ans2 = ""
    for i in range(0, 100):
        bla2 = numpy.reshape(prev6, (1, len(prev6)))
        pred2 = model2.predict(bla2, verbose=0)
        word2 = pred2.argsort()[0][-2:]
        if word2[0] == unk2:
            word2 = word2[1]
        else:
            word2 = word2[0]
        ans2 += convert_int_to_words2[word2] + " "
        prev6 = prev6[1:5]
        prev6 = numpy.append(prev6, word2)
    cnns.append(ans2)
    answers.append("cnn")
```


```python
unk3 = convert_words_to_int3["unknown"]
recipes = []
count_recipe = len(df[df.text == 'recipe'])
for j in range(0, int(0.3*count_recipe)):
    prev7 = np.array([convert_words_to_int3[word] for word in vocab_words3[j*10:j*10+5]])
    ans3 = ""
    for i in range(0, 100):
        bla3 = numpy.reshape(prev7, (1, len(prev7)))
        pred3 = model3.predict(bla3, verbose=0)
        word3 = pred3.argsort()[0][-2:]
        if word3[0] == unk3:
            word3 = word3[1]
        else:
            word3 = word3[0]
        ans3 += convert_int_to_words3[word3] + " "
        prev7 = prev7[1:5]
        prev7 = numpy.append(prev7, word3)
    recipes.append(ans3)
    answers.append("recipe")
```

### now, we load our previous model from part b of the assignment and we will try to classify the data we created 


```python
from sklearn.externals import joblib
import pickle
forest = joblib.load('forest.pkl')
vectorizer = None
with open('vectorizer.pkl', 'rb') as f:
    vectorizer_vocab = pickle.load(f)
```


```python
import nltk
from nltk.corpus import stopwords # Import the stop word list
from sklearn.feature_extraction.text import CountVectorizer

def parse(text):
    words = nltk.word_tokenize(text) # Split into words
    words = [w for w in words if not w in stopwords.words("english")]
    porter = nltk.PorterStemmer()
    [porter.stem(w) for w in words]
    return " ".join(words)

vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             vocabulary = vectorizer_vocab,
                             min_df = 10) 

cleaned_data = [parse(t) for t in wikis]
train_data_features = vectorizer.fit_transform(cleaned_data)
train_data_features = train_data_features.toarray()



```


```python
cleaned_data2 = [parse(t) for t in cnns]
train_data_features2 = vectorizer.fit_transform(cleaned_data2)
train_data_features2 = train_data_features2.toarray()
```


```python
cleaned_data3 = [parse(t) for t in recipes]
train_data_features3 = vectorizer.fit_transform(cleaned_data3)
train_data_features3 = train_data_features3.toarray()
```

## Merge data from cnn, wiki, recipes 


```python
cleaned_data4 = cleaned_data + cleaned_data2 + cleaned_data3
train_data_features4 = vectorizer.fit_transform(cleaned_data4)
train_data_features4 = train_data_features4.toarray()
```

### check the score for each classification


```python
forest.score(train_data_features, ['wiki']*len(cleaned_data))
```




    0.80000000000000004




```python
forest.score(train_data_features2, ['cnn']*len(cleaned_data2))
```




    1.0




```python
forest.score(train_data_features3, ['recipe']*len(cleaned_data3))
```




    0.94117647058823528




```python
final_ans = ["wiki"]*len(cleaned_data) + ["cnn"]*len(cleaned_data2) + ["recipe"]*len(cleaned_data3)
```


```python
print("Accuracy: %0.2f" % forest.score(train_data_features4,final_ans))
```

    Accuracy: 0.91


## wiki - we have some mistakes, maybe because the text is similar to some cnn articles because it has been choose randomaly
## cnn - all true
## recipe, small mistake


```python
from sklearn.metrics import confusion_matrix
prediction = forest.predict(train_data_features4)
confusion_matrix(final_ans, prediction, labels=["wiki", "cnn","recipe"])
```




    array([[12,  3,  0],
           [ 0, 15,  0],
           [ 0,  1, 16]])


