{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment - part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First stage - load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import unicodedata\n",
    "df = pd.read_csv('data_final.csv',header=0, \n",
    "                    delimiter=\",\")\n",
    "#df = df.ix[1:]\n",
    "#df = pd.DataFrame(data=df, columns=['text','type'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 300\n",
    "unknown_token = \"UNKNOWNTOKEN\"\n",
    "sentence_start_token = \"SENTENCESTART\"\n",
    "sentence_end_token = \"SENTENCEEND\"\n",
    "line_break= \"NEWLINE\"\n",
    "separator= \"SEPARATOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afromarengo plana jumping spider species genus afromarengo lives south africa'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in df[\"type\"]:\n",
    "    text1 = d.replace('\\n',' '+ line_break + ' ')\n",
    "    text1 = text1.replace('--',' '+ separator + ' ')\n",
    "    text1 = text1.replace('.',' '+sentence_end_token +' '+ sentence_start_token+' ' )\n",
    "df[\"type\"][2] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into 3 different structures to build the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_data   = df[df[\"text\"] == \"wiki\"]\n",
    "cnn_data    = df[df[\"text\"] == \"cnn\"]\n",
    "recipe_data = df[df[\"text\"] == \"recipe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import keras \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "wiki_string = \"\"\n",
    "for wk in wiki_data[\"type\"]:\n",
    "    wiki_string+= \" \" + wk\n",
    "wiki_words = text_to_word_sequence(wiki_string, lower=False, split=\" \")\n",
    "\n",
    "cnn_string = \"\"\n",
    "for cn in cnn_data[\"type\"]:\n",
    "    cnn_string+= \" \" + cn\n",
    "cnn_words = text_to_word_sequence(cnn_string, lower=False, split=\" \")\n",
    "\n",
    "\n",
    "recipe_string = \"\"\n",
    "for rc in recipe_data[\"type\"]:\n",
    "    recipe_string+= \" \" + rc\n",
    "recipe_words = text_to_word_sequence(recipe_string, lower=False, split=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn vectorizer to create productive vocablury for 3 different list of words(cnn,wiki,recipe)\n",
    "## Make list of lists that for every 5 words knows the 6 one\n",
    "## Using keras library to create sequence of word from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "xData = []\n",
    "yData = []\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features=600\n",
    "                              ) \n",
    "vectorizer.fit_transform(wiki_words)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab.append(\"unknown\")\n",
    "vocab_words = []\n",
    "for w in wiki_words:\n",
    "    if w in vocab:\n",
    "        vocab_words.append(w)\n",
    "    else:\n",
    "        vocab_words.append(\"unknown\")\n",
    "convert_words_to_int = dict()\n",
    "convert_int_to_words = dict()\n",
    "for i, c in enumerate(vocab):\n",
    "    convert_words_to_int[c] = i\n",
    "for i, c in enumerate(vocab):\n",
    "    convert_int_to_words[i] = c\n",
    "num_of_words = len(vocab_words)\n",
    "num_of_vocav = len(vocab)\n",
    "\n",
    "for i in range(0, num_of_words - 5, 1):\n",
    "        list_in = vocab_words[i:i + 5]\n",
    "        list_out = vocab_words[i + 5]\n",
    "        xData.append([convert_words_to_int[word] for word in list_in])\n",
    "        yData.append(convert_words_to_int[list_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xData2 = []\n",
    "yData2 = []\n",
    "vectorizer2 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features=600\n",
    "                              ) \n",
    "vectorizer2.fit_transform(cnn_words)\n",
    "vocab2 = vectorizer2.get_feature_names()\n",
    "vocab2.append(\"unknown\")\n",
    "vocab_words2 = []\n",
    "for w in cnn_words:\n",
    "    if w in vocab2:\n",
    "        vocab_words2.append(w)\n",
    "    else:\n",
    "        vocab_words2.append(\"unknown\")\n",
    "convert_words_to_int2 = dict()\n",
    "convert_int_to_words2 = dict()\n",
    "for i, c in enumerate(vocab2):\n",
    "    convert_words_to_int2[c] = i\n",
    "for i, c in enumerate(vocab2):\n",
    "    convert_int_to_words2[i] = c\n",
    "num_of_words2 = len(vocab_words2)\n",
    "num_of_vocav2 = len(vocab2)\n",
    "\n",
    "for i in range(0, num_of_words2 - 5, 1):\n",
    "        list_in2 = vocab_words2[i:i + 5]\n",
    "        list_out2 = vocab_words2[i + 5]\n",
    "        xData2.append([convert_words_to_int2[word] for word in list_in2])\n",
    "        yData2.append(convert_words_to_int2[list_out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xData3 = []\n",
    "yData3 = []\n",
    "vectorizer3 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features=600\n",
    "                              ) \n",
    "vectorizer3.fit_transform(recipe_words)\n",
    "vocab3 = vectorizer3.get_feature_names()\n",
    "vocab3.append(\"unknown\")\n",
    "vocab_words3 = []\n",
    "for w in recipe_words:\n",
    "    if w in vocab3:\n",
    "        vocab_words3.append(w)\n",
    "    else:\n",
    "        vocab_words3.append(\"unknown\")\n",
    "convert_words_to_int3 = dict()\n",
    "convert_int_to_words3 = dict()\n",
    "for i, c in enumerate(vocab3):\n",
    "    convert_words_to_int3[c] = i\n",
    "for i, c in enumerate(vocab3):\n",
    "    convert_int_to_words3[i] = c\n",
    "num_of_words3 = len(vocab_words3)\n",
    "num_of_vocav3 = len(vocab3)\n",
    "\n",
    "for i in range(0, num_of_words3 - 5, 1):\n",
    "        list_in3 = vocab_words3[i:i + 5]\n",
    "        list_out3 = vocab_words3[i + 5]\n",
    "        xData3.append([convert_words_to_int3[word] for word in list_in3])\n",
    "        yData3.append(convert_words_to_int3[list_out3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3 models for each kind of data\n",
    "## The model we chose is the simple RNN model\n",
    "### 5.Create a model with:\n",
    "### an Embedding input layer with 42 entries\n",
    "### a SimpleRNN layer with 186 hidde nodes and a relu activation function and identity initialization\n",
    "### a Dense output layer with a softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "import numpy\n",
    "from keras.utils import np_utils\n",
    "n_hidden = 186\n",
    "n_fac = 42\n",
    "X = numpy.array(xData)  \n",
    "Y = np_utils.to_categorical(yData) \n",
    "model=Sequential([\n",
    "        Embedding(input_dim=X.shape[0], output_dim=n_fac, input_length=X.shape[1]),\n",
    "        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n",
    "        Dense(Y.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "X2 = numpy.array(xData2)  \n",
    "Y2 = np_utils.to_categorical(yData2) \n",
    "model2=Sequential([\n",
    "        Embedding(input_dim=X2.shape[0], output_dim=n_fac, input_length=X2.shape[1]),\n",
    "        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n",
    "        Dense(Y2.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "X3 = numpy.array(xData3) \n",
    "Y3 = np_utils.to_categorical(yData3) \n",
    "model3=Sequential([\n",
    "        Embedding(input_dim=X3.shape[0], output_dim=n_fac, input_length=X3.shape[1]),\n",
    "        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n",
    "        Dense(Y3.shape[1], activation='softmax')\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the summary of the 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 5, 42)         108570      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_1 (SimpleRNN)          (None, 186)           42594       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 601)           112387      simplernn_1[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 263,551\n",
      "Trainable params: 263,551\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 5, 42)         118566      embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_2 (SimpleRNN)          (None, 186)           42594       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 601)           112387      simplernn_2[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 273,547\n",
      "Trainable params: 273,547\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 5, 42)         261828      embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_3 (SimpleRNN)          (None, 186)           42594       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 601)           112387      simplernn_3[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 416,809\n",
      "Trainable params: 416,809\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model2.summary()\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the models and fit them to the data for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2585/2585 [==============================] - 0s - loss: 5.4738 - acc: 0.3110     \n",
      "Epoch 2/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.8058 - acc: 0.3412     \n",
      "Epoch 3/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.7751 - acc: 0.3412     \n",
      "Epoch 4/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.7125 - acc: 0.3412     \n",
      "Epoch 5/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.6778 - acc: 0.3412     \n",
      "Epoch 6/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.7016 - acc: 0.3412     \n",
      "Epoch 7/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.6526 - acc: 0.3412     \n",
      "Epoch 8/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.6204 - acc: 0.3412     \n",
      "Epoch 9/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.5910 - acc: 0.3412     \n",
      "Epoch 10/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.5938 - acc: 0.3412     \n",
      "Epoch 11/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.5273 - acc: 0.3412     \n",
      "Epoch 12/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.5065 - acc: 0.3412     \n",
      "Epoch 13/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.4772 - acc: 0.3416     \n",
      "Epoch 14/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.4410 - acc: 0.3427     \n",
      "Epoch 15/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.3773 - acc: 0.3427     \n",
      "Epoch 16/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.3395 - acc: 0.3431     \n",
      "Epoch 17/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.3520 - acc: 0.3435     \n",
      "Epoch 18/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.2881 - acc: 0.3435     \n",
      "Epoch 19/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.2626 - acc: 0.3431     \n",
      "Epoch 20/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.2206 - acc: 0.3439     \n",
      "Epoch 21/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.1945 - acc: 0.3443     \n",
      "Epoch 22/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.1827 - acc: 0.3455     \n",
      "Epoch 23/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.1325 - acc: 0.3462     \n",
      "Epoch 24/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.1101 - acc: 0.3458     \n",
      "Epoch 25/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.0776 - acc: 0.3470     \n",
      "Epoch 26/100\n",
      "2585/2585 [==============================] - 0s - loss: 4.0031 - acc: 0.3474     \n",
      "Epoch 27/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.9466 - acc: 0.3497     \n",
      "Epoch 28/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.9415 - acc: 0.3493     \n",
      "Epoch 29/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.8701 - acc: 0.3497     \n",
      "Epoch 30/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.7999 - acc: 0.3540     \n",
      "Epoch 31/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.7780 - acc: 0.3509     \n",
      "Epoch 32/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.6536 - acc: 0.3574     \n",
      "Epoch 33/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.6359 - acc: 0.3563     \n",
      "Epoch 34/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.5476 - acc: 0.3594     \n",
      "Epoch 35/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.4753 - acc: 0.3660     \n",
      "Epoch 36/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.3713 - acc: 0.3714     \n",
      "Epoch 37/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.2867 - acc: 0.3725     \n",
      "Epoch 38/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.2453 - acc: 0.3718     \n",
      "Epoch 39/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.1313 - acc: 0.3783     \n",
      "Epoch 40/100\n",
      "2585/2585 [==============================] - 0s - loss: 3.0241 - acc: 0.3876     \n",
      "Epoch 41/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.9494 - acc: 0.3985     \n",
      "Epoch 42/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.8652 - acc: 0.4043     \n",
      "Epoch 43/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.7754 - acc: 0.4062     \n",
      "Epoch 44/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.6576 - acc: 0.4294     \n",
      "Epoch 45/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.6023 - acc: 0.4368     \n",
      "Epoch 46/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.5275 - acc: 0.4429     \n",
      "Epoch 47/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.4484 - acc: 0.4573     \n",
      "Epoch 48/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.3383 - acc: 0.4770     \n",
      "Epoch 49/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.2909 - acc: 0.4785     \n",
      "Epoch 50/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.1893 - acc: 0.4940     \n",
      "Epoch 51/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.1246 - acc: 0.5072     \n",
      "Epoch 52/100\n",
      "2585/2585 [==============================] - 0s - loss: 2.0663 - acc: 0.5215     \n",
      "Epoch 53/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.9764 - acc: 0.5323     \n",
      "Epoch 54/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.9023 - acc: 0.5462     \n",
      "Epoch 55/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.8197 - acc: 0.5567     \n",
      "Epoch 56/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.7847 - acc: 0.5737     \n",
      "Epoch 57/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.6916 - acc: 0.5830     \n",
      "Epoch 58/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.5985 - acc: 0.6124     \n",
      "Epoch 59/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.6273 - acc: 0.6043     \n",
      "Epoch 60/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.5569 - acc: 0.6143     \n",
      "Epoch 61/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.4064 - acc: 0.6538     \n",
      "Epoch 62/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.4708 - acc: 0.6344     \n",
      "Epoch 63/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.3722 - acc: 0.6530     \n",
      "Epoch 64/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.3404 - acc: 0.6685     \n",
      "Epoch 65/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.2567 - acc: 0.6909     \n",
      "Epoch 66/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.2432 - acc: 0.6886     \n",
      "Epoch 67/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.2063 - acc: 0.7037     \n",
      "Epoch 68/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.1849 - acc: 0.6979     \n",
      "Epoch 69/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.1150 - acc: 0.7296     \n",
      "Epoch 70/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.0558 - acc: 0.7373     \n",
      "Epoch 71/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.0864 - acc: 0.7188     \n",
      "Epoch 72/100\n",
      "2585/2585 [==============================] - 0s - loss: 1.0649 - acc: 0.7234     \n",
      "Epoch 73/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.8922 - acc: 0.7783     \n",
      "Epoch 74/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.9768 - acc: 0.7513     \n",
      "Epoch 75/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.9361 - acc: 0.7574     \n",
      "Epoch 76/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.8548 - acc: 0.7818     \n",
      "Epoch 77/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.9397 - acc: 0.7675     \n",
      "Epoch 78/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.7952 - acc: 0.8066     \n",
      "Epoch 79/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.7907 - acc: 0.8066     \n",
      "Epoch 80/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.8688 - acc: 0.7787     \n",
      "Epoch 81/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.7066 - acc: 0.8244     \n",
      "Epoch 82/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.8011 - acc: 0.7919     \n",
      "Epoch 83/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.7140 - acc: 0.8209     \n",
      "Epoch 84/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6928 - acc: 0.8302     \n",
      "Epoch 85/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.7191 - acc: 0.8159     \n",
      "Epoch 86/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6541 - acc: 0.8464     \n",
      "Epoch 87/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6165 - acc: 0.8545     \n",
      "Epoch 88/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.7395 - acc: 0.8031     \n",
      "Epoch 89/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.5745 - acc: 0.8611     \n",
      "Epoch 90/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6353 - acc: 0.8391     \n",
      "Epoch 91/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.5666 - acc: 0.8596     \n",
      "Epoch 92/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.5630 - acc: 0.8573     \n",
      "Epoch 93/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6616 - acc: 0.8275     \n",
      "Epoch 94/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.4921 - acc: 0.8824     \n",
      "Epoch 95/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.5188 - acc: 0.8700     \n",
      "Epoch 96/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6283 - acc: 0.8251     \n",
      "Epoch 97/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.4497 - acc: 0.8928     \n",
      "Epoch 98/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.6002 - acc: 0.8460     \n",
      "Epoch 99/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.4605 - acc: 0.8836     \n",
      "Epoch 100/100\n",
      "2585/2585 [==============================] - 0s - loss: 0.4176 - acc: 0.9002     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11740a990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y=Y, batch_size=200, nb_epoch=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2823/2823 [==============================] - 0s - loss: 5.8394 - acc: 0.1923     \n",
      "Epoch 2/100\n",
      "2823/2823 [==============================] - 0s - loss: 5.3878 - acc: 0.2111     \n",
      "Epoch 3/100\n",
      "2823/2823 [==============================] - 0s - loss: 5.2779 - acc: 0.2111     \n",
      "Epoch 4/100\n",
      "2823/2823 [==============================] - 0s - loss: 5.2020 - acc: 0.2111     \n",
      "Epoch 5/100\n",
      "2823/2823 [==============================] - 0s - loss: 5.0985 - acc: 0.2129     \n",
      "Epoch 6/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.9147 - acc: 0.2193     \n",
      "Epoch 7/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.7981 - acc: 0.2196     \n",
      "Epoch 8/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.6732 - acc: 0.2256     \n",
      "Epoch 9/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.5498 - acc: 0.2299     \n",
      "Epoch 10/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.4174 - acc: 0.2370     \n",
      "Epoch 11/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.2448 - acc: 0.2490     \n",
      "Epoch 12/100\n",
      "2823/2823 [==============================] - 0s - loss: 4.0266 - acc: 0.2678     \n",
      "Epoch 13/100\n",
      "2823/2823 [==============================] - 0s - loss: 3.8746 - acc: 0.2859     \n",
      "Epoch 14/100\n",
      "2823/2823 [==============================] - 0s - loss: 3.6011 - acc: 0.3181     \n",
      "Epoch 15/100\n",
      "2823/2823 [==============================] - 0s - loss: 3.4047 - acc: 0.3482     \n",
      "Epoch 16/100\n",
      "2823/2823 [==============================] - 0s - loss: 3.1750 - acc: 0.3826     \n",
      "Epoch 17/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.9483 - acc: 0.4166     \n",
      "Epoch 18/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.7642 - acc: 0.4573     \n",
      "Epoch 19/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.6665 - acc: 0.4782     \n",
      "Epoch 20/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.4757 - acc: 0.5097     \n",
      "Epoch 21/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.3343 - acc: 0.5345     \n",
      "Epoch 22/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.2532 - acc: 0.5515     \n",
      "Epoch 23/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.1087 - acc: 0.5859     \n",
      "Epoch 24/100\n",
      "2823/2823 [==============================] - 0s - loss: 2.0438 - acc: 0.5831     \n",
      "Epoch 25/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.9456 - acc: 0.6114     \n",
      "Epoch 26/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.8479 - acc: 0.6259     \n",
      "Epoch 27/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.8202 - acc: 0.6242     \n",
      "Epoch 28/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.7438 - acc: 0.6440     \n",
      "Epoch 29/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.6638 - acc: 0.6635     \n",
      "Epoch 30/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.5915 - acc: 0.6674     \n",
      "Epoch 31/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.5915 - acc: 0.6585     \n",
      "Epoch 32/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.5046 - acc: 0.6805     \n",
      "Epoch 33/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.4403 - acc: 0.6872     \n",
      "Epoch 34/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.4134 - acc: 0.6915     \n",
      "Epoch 35/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.3641 - acc: 0.7028     \n",
      "Epoch 36/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.3557 - acc: 0.6900     \n",
      "Epoch 37/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.2714 - acc: 0.7145     \n",
      "Epoch 38/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.2845 - acc: 0.7102     \n",
      "Epoch 39/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.2058 - acc: 0.7226     \n",
      "Epoch 40/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.2664 - acc: 0.6996     \n",
      "Epoch 41/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.1232 - acc: 0.7407     \n",
      "Epoch 42/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.1338 - acc: 0.7318     \n",
      "Epoch 43/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.0752 - acc: 0.7403     \n",
      "Epoch 44/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.1422 - acc: 0.7180     \n",
      "Epoch 45/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.0238 - acc: 0.7531     \n",
      "Epoch 46/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.0463 - acc: 0.7411     \n",
      "Epoch 47/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.9879 - acc: 0.7549     \n",
      "Epoch 48/100\n",
      "2823/2823 [==============================] - 0s - loss: 1.0542 - acc: 0.7403     \n",
      "Epoch 49/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.9422 - acc: 0.7627     \n",
      "Epoch 50/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.9076 - acc: 0.7736     \n",
      "Epoch 51/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.9951 - acc: 0.7474     \n",
      "Epoch 52/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8655 - acc: 0.7768     \n",
      "Epoch 53/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8801 - acc: 0.7800     \n",
      "Epoch 54/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8339 - acc: 0.7853     \n",
      "Epoch 55/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8363 - acc: 0.7843     \n",
      "Epoch 56/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8591 - acc: 0.7804     \n",
      "Epoch 57/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8028 - acc: 0.7889     \n",
      "Epoch 58/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.8246 - acc: 0.7790     \n",
      "Epoch 59/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.7672 - acc: 0.8006     \n",
      "Epoch 60/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.7094 - acc: 0.8091     \n",
      "Epoch 61/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.7936 - acc: 0.7871     \n",
      "Epoch 62/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6969 - acc: 0.8130     \n",
      "Epoch 63/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.7482 - acc: 0.8002     \n",
      "Epoch 64/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6854 - acc: 0.8197     \n",
      "Epoch 65/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6782 - acc: 0.8144     \n",
      "Epoch 66/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6633 - acc: 0.8211     \n",
      "Epoch 67/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6597 - acc: 0.8264     \n",
      "Epoch 68/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6456 - acc: 0.8278     \n",
      "Epoch 69/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6077 - acc: 0.8328     \n",
      "Epoch 70/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6000 - acc: 0.8371     \n",
      "Epoch 71/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.5901 - acc: 0.8409     \n",
      "Epoch 72/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6222 - acc: 0.8296     \n",
      "Epoch 73/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.5574 - acc: 0.8480     \n",
      "Epoch 74/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6032 - acc: 0.8339     \n",
      "Epoch 75/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.5495 - acc: 0.8498     \n",
      "Epoch 76/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.6072 - acc: 0.8378     \n",
      "Epoch 77/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.5099 - acc: 0.8622     \n",
      "Epoch 78/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.5577 - acc: 0.8445     \n",
      "Epoch 79/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.5572 - acc: 0.8498     \n",
      "Epoch 80/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4745 - acc: 0.8686     \n",
      "Epoch 81/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4999 - acc: 0.8590     \n",
      "Epoch 82/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4837 - acc: 0.8618     \n",
      "Epoch 83/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4826 - acc: 0.8654     \n",
      "Epoch 84/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4906 - acc: 0.8622     \n",
      "Epoch 85/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4504 - acc: 0.8746     \n",
      "Epoch 86/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4688 - acc: 0.8672     \n",
      "Epoch 87/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4306 - acc: 0.8813     \n",
      "Epoch 88/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4973 - acc: 0.8523     \n",
      "Epoch 89/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4109 - acc: 0.8845     \n",
      "Epoch 90/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4262 - acc: 0.8771     \n",
      "Epoch 91/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4074 - acc: 0.8785     \n",
      "Epoch 92/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.4278 - acc: 0.8704     \n",
      "Epoch 93/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3914 - acc: 0.8895     \n",
      "Epoch 94/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3893 - acc: 0.8852     \n",
      "Epoch 95/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3853 - acc: 0.8913     \n",
      "Epoch 96/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3806 - acc: 0.8927     \n",
      "Epoch 97/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3795 - acc: 0.8930     \n",
      "Epoch 98/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3744 - acc: 0.8920     \n",
      "Epoch 99/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3717 - acc: 0.8891     \n",
      "Epoch 100/100\n",
      "2823/2823 [==============================] - 0s - loss: 0.3443 - acc: 0.9012     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12140cb10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X2, y=Y2, batch_size=200, nb_epoch=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6234/6234 [==============================] - 0s - loss: 5.0379 - acc: 0.1833     \n",
      "Epoch 2/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.7024 - acc: 0.1888     \n",
      "Epoch 3/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.6433 - acc: 0.1888     \n",
      "Epoch 4/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.5834 - acc: 0.1878     \n",
      "Epoch 5/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.4776 - acc: 0.1893     \n",
      "Epoch 6/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.3784 - acc: 0.1960     \n",
      "Epoch 7/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.2683 - acc: 0.1970     \n",
      "Epoch 8/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.1895 - acc: 0.1996     \n",
      "Epoch 9/100\n",
      "6234/6234 [==============================] - 0s - loss: 4.0878 - acc: 0.2056     \n",
      "Epoch 10/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.9989 - acc: 0.2114     \n",
      "Epoch 11/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.9002 - acc: 0.2239     \n",
      "Epoch 12/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.8163 - acc: 0.2324     \n",
      "Epoch 13/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.7257 - acc: 0.2488     \n",
      "Epoch 14/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.6217 - acc: 0.2546     \n",
      "Epoch 15/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.5311 - acc: 0.2640     \n",
      "Epoch 16/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.4488 - acc: 0.2719     \n",
      "Epoch 17/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.3566 - acc: 0.2807     \n",
      "Epoch 18/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.2678 - acc: 0.2915     \n",
      "Epoch 19/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.1787 - acc: 0.3009     \n",
      "Epoch 20/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.0919 - acc: 0.3131     \n",
      "Epoch 21/100\n",
      "6234/6234 [==============================] - 0s - loss: 3.0034 - acc: 0.3194     \n",
      "Epoch 22/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.9069 - acc: 0.3306     \n",
      "Epoch 23/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.8210 - acc: 0.3431     \n",
      "Epoch 24/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.7340 - acc: 0.3503     \n",
      "Epoch 25/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.6417 - acc: 0.3657     \n",
      "Epoch 26/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.5541 - acc: 0.3712     \n",
      "Epoch 27/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.4645 - acc: 0.3879     \n",
      "Epoch 28/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.3894 - acc: 0.3999     \n",
      "Epoch 29/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.3009 - acc: 0.4113     \n",
      "Epoch 30/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.2246 - acc: 0.4256     \n",
      "Epoch 31/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.1555 - acc: 0.4406     \n",
      "Epoch 32/100\n",
      "6234/6234 [==============================] - 0s - loss: 2.0656 - acc: 0.4613     \n",
      "Epoch 33/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.9995 - acc: 0.4764     \n",
      "Epoch 34/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.9267 - acc: 0.4859     \n",
      "Epoch 35/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.8573 - acc: 0.5077     \n",
      "Epoch 36/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.7975 - acc: 0.5172     \n",
      "Epoch 37/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.7244 - acc: 0.5417     \n",
      "Epoch 38/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.6843 - acc: 0.5465     \n",
      "Epoch 39/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.6287 - acc: 0.5566     \n",
      "Epoch 40/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.5740 - acc: 0.5687     \n",
      "Epoch 41/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.5168 - acc: 0.5831     \n",
      "Epoch 42/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.4706 - acc: 0.5946     \n",
      "Epoch 43/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.4233 - acc: 0.6036     \n",
      "Epoch 44/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.3807 - acc: 0.6182     \n",
      "Epoch 45/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.3400 - acc: 0.6291     \n",
      "Epoch 46/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.2793 - acc: 0.6444     \n",
      "Epoch 47/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.2654 - acc: 0.6432     \n",
      "Epoch 48/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.2046 - acc: 0.6517     \n",
      "Epoch 49/100\n",
      "6234/6234 [==============================] - 1s - loss: 1.1646 - acc: 0.6707     \n",
      "Epoch 50/100\n",
      "6234/6234 [==============================] - 1s - loss: 1.1299 - acc: 0.6739     \n",
      "Epoch 51/100\n",
      "6234/6234 [==============================] - 1s - loss: 1.0900 - acc: 0.6824     \n",
      "Epoch 52/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.0716 - acc: 0.6869     \n",
      "Epoch 53/100\n",
      "6234/6234 [==============================] - 0s - loss: 1.0279 - acc: 0.7098     \n",
      "Epoch 54/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.9860 - acc: 0.7185     \n",
      "Epoch 55/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.9659 - acc: 0.7190     \n",
      "Epoch 56/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.9348 - acc: 0.7350     \n",
      "Epoch 57/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.8851 - acc: 0.7406     \n",
      "Epoch 58/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.8700 - acc: 0.7421     \n",
      "Epoch 59/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.8492 - acc: 0.7483     \n",
      "Epoch 60/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.8119 - acc: 0.7640     \n",
      "Epoch 61/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.7920 - acc: 0.7648     \n",
      "Epoch 62/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.7689 - acc: 0.7708     \n",
      "Epoch 63/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.7357 - acc: 0.7839     \n",
      "Epoch 64/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.7347 - acc: 0.7807     \n",
      "Epoch 65/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6939 - acc: 0.7913     \n",
      "Epoch 66/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6758 - acc: 0.7980     \n",
      "Epoch 67/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6681 - acc: 0.7977     \n",
      "Epoch 68/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6446 - acc: 0.8033     \n",
      "Epoch 69/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6195 - acc: 0.8189     \n",
      "Epoch 70/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6067 - acc: 0.8186     \n",
      "Epoch 71/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.6002 - acc: 0.8149     \n",
      "Epoch 72/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.5616 - acc: 0.8308     \n",
      "Epoch 73/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.5678 - acc: 0.8304     \n",
      "Epoch 74/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.5476 - acc: 0.8325     \n",
      "Epoch 75/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.5260 - acc: 0.8410     \n",
      "Epoch 76/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.5132 - acc: 0.8489     \n",
      "Epoch 77/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.5110 - acc: 0.8447     \n",
      "Epoch 78/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4989 - acc: 0.8484     \n",
      "Epoch 79/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4798 - acc: 0.8521     \n",
      "Epoch 80/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4744 - acc: 0.8551     \n",
      "Epoch 81/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4591 - acc: 0.8564     \n",
      "Epoch 82/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4492 - acc: 0.8630     \n",
      "Epoch 83/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4369 - acc: 0.8649     \n",
      "Epoch 84/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4311 - acc: 0.8648     \n",
      "Epoch 85/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4157 - acc: 0.8720     \n",
      "Epoch 86/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.4249 - acc: 0.8672     \n",
      "Epoch 87/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3926 - acc: 0.8799     \n",
      "Epoch 88/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3964 - acc: 0.8744     \n",
      "Epoch 89/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3871 - acc: 0.8805     \n",
      "Epoch 90/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3830 - acc: 0.8762     \n",
      "Epoch 91/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3740 - acc: 0.8811     \n",
      "Epoch 92/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3708 - acc: 0.8802     \n",
      "Epoch 93/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3585 - acc: 0.8848     \n",
      "Epoch 94/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3558 - acc: 0.8906     \n",
      "Epoch 95/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3433 - acc: 0.8898     \n",
      "Epoch 96/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3406 - acc: 0.8914     \n",
      "Epoch 97/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3365 - acc: 0.8924     \n",
      "Epoch 98/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3314 - acc: 0.8927     \n",
      "Epoch 99/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3258 - acc: 0.8909     \n",
      "Epoch 100/100\n",
      "6234/6234 [==============================] - 1s - loss: 0.3216 - acc: 0.8948     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121e62310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X3, y=Y3, batch_size=200, nb_epoch=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genarate texts from each data using the models we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "unk = convert_words_to_int[\"unknown\"]\n",
    "wikis = []\n",
    "count_wiki = len(df[df.text == 'wiki'])\n",
    "for j in range(0, int(0.3*count_wiki)):\n",
    "    prev5 = np.array([convert_words_to_int[word] for word in vocab_words[j*10:j*10+5]])\n",
    "    ans = \"\"\n",
    "    for i in range(0, 100):\n",
    "        bla = numpy.reshape(prev5, (1, len(prev5)))\n",
    "        pred = model.predict(bla, verbose=0)\n",
    "        word1 = pred.argsort()[0][-2:]\n",
    "        # print (word1[0])\n",
    "        if word1[0] == unk:\n",
    "            word1 = word1[1]\n",
    "        else:\n",
    "            word1 = word1[0]\n",
    "        # print(convert_int_to_words[word1])\n",
    "        ans += convert_int_to_words[word1] + \" \"\n",
    "        prev5 = prev5[1:5]\n",
    "        prev5 = numpy.append(prev5, word1)\n",
    "    wikis.append(ans)\n",
    "    answers.append(\"wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unk2 = convert_words_to_int2[\"unknown\"]\n",
    "cnns = []\n",
    "count_cnn = len(df[df.text == 'cnn'])\n",
    "for j in range(0, int(0.3*count_cnn)):\n",
    "    prev6 = np.array([convert_words_to_int2[word] for word in vocab_words2[j*10:j*10+5]])\n",
    "    ans2 = \"\"\n",
    "    for i in range(0, 100):\n",
    "        bla2 = numpy.reshape(prev6, (1, len(prev6)))\n",
    "        pred2 = model2.predict(bla2, verbose=0)\n",
    "        word2 = pred2.argsort()[0][-2:]\n",
    "        if word2[0] == unk2:\n",
    "            word2 = word2[1]\n",
    "        else:\n",
    "            word2 = word2[0]\n",
    "        ans2 += convert_int_to_words2[word2] + \" \"\n",
    "        prev6 = prev6[1:5]\n",
    "        prev6 = numpy.append(prev6, word2)\n",
    "    cnns.append(ans2)\n",
    "    answers.append(\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unk3 = convert_words_to_int3[\"unknown\"]\n",
    "recipes = []\n",
    "count_recipe = len(df[df.text == 'recipe'])\n",
    "for j in range(0, int(0.3*count_recipe)):\n",
    "    prev7 = np.array([convert_words_to_int3[word] for word in vocab_words3[j*10:j*10+5]])\n",
    "    ans3 = \"\"\n",
    "    for i in range(0, 100):\n",
    "        bla3 = numpy.reshape(prev7, (1, len(prev7)))\n",
    "        pred3 = model3.predict(bla3, verbose=0)\n",
    "        word3 = pred3.argsort()[0][-2:]\n",
    "        if word3[0] == unk3:\n",
    "            word3 = word3[1]\n",
    "        else:\n",
    "            word3 = word3[0]\n",
    "        ans3 += convert_int_to_words3[word3] + \" \"\n",
    "        prev7 = prev7[1:5]\n",
    "        prev7 = numpy.append(prev7, word3)\n",
    "    recipes.append(ans3)\n",
    "    answers.append(\"recipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now, we load our previous model from part b of the assignment and we will try to classify the data we created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "forest = joblib.load('forest.pkl')\n",
    "vectorizer = None\n",
    "with open('vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def parse(text):\n",
    "    words = nltk.word_tokenize(text) # Split into words\n",
    "    words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "    porter = nltk.PorterStemmer()\n",
    "    [porter.stem(w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             vocabulary = vectorizer_vocab,\n",
    "                             min_df = 10) \n",
    "\n",
    "cleaned_data = [parse(t) for t in wikis]\n",
    "train_data_features = vectorizer.fit_transform(cleaned_data)\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_data2 = [parse(t) for t in cnns]\n",
    "train_data_features2 = vectorizer.fit_transform(cleaned_data2)\n",
    "train_data_features2 = train_data_features2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_data3 = [parse(t) for t in recipes]\n",
    "train_data_features3 = vectorizer.fit_transform(cleaned_data3)\n",
    "train_data_features3 = train_data_features3.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data from cnn, wiki, recipes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_data4 = cleaned_data + cleaned_data2 + cleaned_data3\n",
    "train_data_features4 = vectorizer.fit_transform(cleaned_data4)\n",
    "train_data_features4 = train_data_features4.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the score for each classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(train_data_features, ['wiki']*len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(train_data_features2, ['cnn']*len(cleaned_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94117647058823528"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(train_data_features3, ['recipe']*len(cleaned_data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_ans = [\"wiki\"]*len(cleaned_data) + [\"cnn\"]*len(cleaned_data2) + [\"recipe\"]*len(cleaned_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f\" % forest.score(train_data_features4,final_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wiki - we have some mistakes, maybe because the text is similar to some cnn articles because it has been choose randomaly\n",
    "## cnn - all true\n",
    "## recipe, small mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  3,  0],\n",
       "       [ 0, 15,  0],\n",
       "       [ 0,  1, 16]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "prediction = forest.predict(train_data_features4)\n",
    "confusion_matrix(final_ans, prediction, labels=[\"wiki\", \"cnn\",\"recipe\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
